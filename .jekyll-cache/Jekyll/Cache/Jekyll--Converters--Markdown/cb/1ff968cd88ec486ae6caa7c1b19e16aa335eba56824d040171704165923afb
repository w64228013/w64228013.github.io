I"‹/<blockquote>
  <p>æœ¬æ–‡ä»‹ç»tensorflowä¸­çš„ä¸€äº›åŸºæœ¬æ¦‚å¿µï¼Œå¹¶è¯´æ˜å¦‚ä½•å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹</p>
  <h4 id="variable">Variable</h4>
  <p>åœ¨ Tensorflow ä¸­ï¼Œå®šä¹‰äº†æŸå­—ç¬¦ä¸²æ˜¯å˜é‡ï¼Œå®ƒæ‰æ˜¯å˜é‡ï¼Œè¿™ä¸€ç‚¹æ˜¯ä¸ Python æ‰€ä¸åŒçš„ã€‚</p>
</blockquote>

<p>å®šä¹‰è¯­æ³•ï¼š <code class="language-plaintext highlighter-rouge">state = tf.Variable()</code></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import tensorflow as tf

state = tf.Variable(0, name='counter')

# å®šä¹‰å¸¸é‡ one
one = tf.constant(1)

# å®šä¹‰åŠ æ³•æ­¥éª¤ (æ³¨: æ­¤æ­¥å¹¶æ²¡æœ‰ç›´æ¥è®¡ç®—)
new_value = tf.add(state, one)

# å°† State æ›´æ–°æˆ new_value
update = tf.assign(state, new_value)
</code></pre></div></div>
<p><strong>å¦‚æœä½ åœ¨ Tensorflow ä¸­è®¾å®šäº†å˜é‡ï¼Œé‚£ä¹ˆåˆå§‹åŒ–å˜é‡æ˜¯æœ€é‡è¦çš„</strong>! æ‰€ä»¥å®šä¹‰äº†å˜é‡ä»¥å, ä¸€å®šè¦å®šä¹‰ <code class="language-plaintext highlighter-rouge">init = tf.initialize_all_variables()</code> .</p>

<p>åˆ°è¿™é‡Œå˜é‡è¿˜æ˜¯æ²¡æœ‰è¢«æ¿€æ´»ï¼Œéœ€è¦å†åœ¨ sess é‡Œ, sess.run(init) , æ¿€æ´» init è¿™ä¸€æ­¥.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># å¦‚æœå®šä¹‰ Variable, å°±ä¸€å®šè¦ initialize
# init = tf.initialize_all_variables() # tf é©¬ä¸Šå°±è¦åºŸå¼ƒè¿™ç§å†™æ³•
init = tf.global_variables_initializer()  # æ›¿æ¢æˆè¿™æ ·å°±å¥½
 
# ä½¿ç”¨ Session
with tf.Session() as sess:
    sess.run(init)
    for _ in range(3):
        sess.run(update)
        print(sess.run(state))
</code></pre></div></div>
<p>æ³¨æ„ï¼šç›´æ¥ print(state) ä¸èµ·ä½œç”¨ï¼ï¼</p>

<p>ä¸€å®šè¦æŠŠ sess çš„æŒ‡é’ˆæŒ‡å‘ state å†è¿›è¡Œ print æ‰èƒ½å¾—åˆ°æƒ³è¦çš„ç»“æœï¼</p>

<hr />

<h4 id="placeholder">placeholder</h4>
<p><code class="language-plaintext highlighter-rouge">placeholder</code> æ˜¯ Tensorflow ä¸­çš„å ä½ç¬¦ï¼Œæš‚æ—¶å‚¨å­˜å˜é‡.
Tensorflow å¦‚æœæƒ³è¦<strong>ä»å¤–éƒ¨ä¼ å…¥data, é‚£å°±éœ€è¦ç”¨åˆ° <code class="language-plaintext highlighter-rouge">tf.placeholder()</code></strong>, ç„¶åä»¥è¿™ç§å½¢å¼ä¼ è¾“æ•°æ® <code class="language-plaintext highlighter-rouge">sess.run(***, feed_dict={input: **})</code>.</p>

<p>ç¤ºä¾‹ï¼š</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import tensorflow as tf

#åœ¨ Tensorflow ä¸­éœ€è¦å®šä¹‰ placeholder çš„ type ï¼Œä¸€èˆ¬ä¸º float32 å½¢å¼
input1 = tf.placeholder(tf.float32)
input2 = tf.placeholder(tf.float32)

# mul = multiply æ˜¯å°†input1å’Œinput2 åšä¹˜æ³•è¿ç®—ï¼Œå¹¶è¾“å‡ºä¸º output 
ouput = tf.multiply(input1, input2)
</code></pre></div></div>

<p>æ¥ä¸‹æ¥, <strong>ä¼ å€¼çš„å·¥ä½œäº¤ç»™äº† <code class="language-plaintext highlighter-rouge">sess.run()</code></strong> , éœ€è¦ä¼ å…¥çš„å€¼æ”¾åœ¨äº†<code class="language-plaintext highlighter-rouge">feed_dict={}</code> å¹¶ä¸€ä¸€å¯¹åº”æ¯ä¸€ä¸ª <code class="language-plaintext highlighter-rouge">input. placeholder</code> ä¸ <code class="language-plaintext highlighter-rouge">feed_dict={}</code> æ˜¯ç»‘å®šåœ¨ä¸€èµ·å‡ºç°çš„ã€‚</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>with tf.Session() as sess:
    print(sess.run(ouput, feed_dict={input1: [7.], input2: [2.]}))
# [ 14.]
</code></pre></div></div>
<hr />

<h4 id="add_layer">add_layer</h4>
<p>å®šä¹‰ä¸€ä¸ªæ·»åŠ ç¥ç»å±‚çš„å‡½æ•°å¯¹ä¸ä»¥åæ„å»ºç¥ç»ç½‘ç»œä¼šæœ‰å¾ˆå¤§çš„ä¾¿åˆ©æ€§ã€‚
ç¥ç»å±‚é‡Œå¸¸è§çš„å‚æ•°æœ‰weightsï¼Œbiaseså’Œæ¿€æ´»å‡½æ•°ã€‚
å‡½æ•°å‚æ•°åŒ…æ‹¬è¾“å…¥å€¼ä»¥åŠè¾“å…¥å€¼çš„å¤§å°ï¼Œè¾“å‡ºçš„å¤§å°ä»¥åŠæ¿€æ´»å‡½æ•°ã€‚
åˆå§‹åŒ–çš„æ—¶å€™weightséšæœºç”Ÿæˆï¼Œbiasesä¸æ¨èä¸º0ï¼Œæ‰€ä»¥åŠ 0.1ã€‚
wx_plus_bè¡¨ç¤ºæœªæ¿€æ´»çš„è¾“å‡ºå€¼ã€‚</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def add_layer(inputs, in_size, out_size, activation_function=None):
    Weights = tf.Variable(tf.random_normal([in_size, out_size]))
    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)
    Wx_plus_b = tf.matmul(inputs, Weights) + biases
    if activation_function is None:
        outputs = Wx_plus_b
    else:
        outputs = activation_function(Wx_plus_b)
    return outputs
</code></pre></div></div>
<hr />

<h4 id="ä¸€å…ƒäºŒæ¬¡å‡½æ•°çš„è®­ç»ƒ">ä¸€å…ƒäºŒæ¬¡å‡½æ•°çš„è®­ç»ƒ</h4>

<p>å®šä¹‰ä¸€ä¸ªå•è¾“å…¥å•è¾“å‡ºï¼Œéšè—å±‚åŒ…å«åä¸ªç¥ç»å…ƒçš„ç½‘ç»œç»“æ„è¿›è¡Œè®­ç»ƒã€‚</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import tensorflow as tf
import numpy as np

def add_layer(inputs, in_size, out_size, activation_function=None):
    Weights = tf.Variable(tf.random_normal([in_size, out_size]))
    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)
    Wx_plus_b = tf.matmul(inputs, Weights) + biases
    if activation_function is None:
        outputs = Wx_plus_b
    else:
        outputs = activation_function(Wx_plus_b)
    return outputs

#å¯¼å…¥æ•°æ®ï¼Œå¹¶åŠ å…¥noise
x_data = np.linspace(-1,1,300, dtype=np.float32)[:, np.newaxis]
noise = np.random.normal(0, 0.05, x_data.shape).astype(np.float32)
y_data = np.square(x_data) - 0.5 + noise

#æ•°æ®ä»¥placeholderæ ¼å¼è¾“å…¥ï¼Œ1è¡¨ç¤ºåªæœ‰ä¸€ä¸ªç‰¹å¾ï¼ŒNoneè¡¨ç¤ºä¸é™å¤§å°
xs = tf.placeholder(tf.float32, [None, 1])
ys = tf.placeholder(tf.float32, [None, 1])

 
#æ„å»º1*10*1çš„ç¥ç»ç½‘ç»œ
l1 = add_layer(xs, 1, 10, activation_function=tf.nn.relu)  
prediction = add_layer(l1, 10, 1, activation_function=None)

loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction),
                     reduction_indices=[1]))

train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)

init = tf.global_variables_initializer()  # æ›¿æ¢æˆè¿™æ ·å°±å¥½

sess = tf.Session()
sess.run(init)

for i in range(1000):
    # training
    sess.run(train_step, feed_dict={xs: x_data, ys: y_data})
    if i % 50 == 0:
        # to see the step improvement
        print(sess.run(loss, feed_dict={xs: x_data, ys: y_data}))
</code></pre></div></div>

<h2 id="forå¾ªç¯é‡Œé¢xså¦‚ä½•æ›´æ–°çš„--forè¡¨ç¤ºè®­ç»ƒè½®æ•°æ¯ä¸€è½®éƒ½æŠŠæ‰€æœ‰çš„æ•°æ®ç»™å¤„ç†å®Œ">forå¾ªç¯é‡Œé¢xså¦‚ä½•æ›´æ–°çš„?  <strong>forè¡¨ç¤ºè®­ç»ƒè½®æ•°ï¼Œæ¯ä¸€è½®éƒ½æŠŠæ‰€æœ‰çš„æ•°æ®ç»™å¤„ç†å®Œã€‚</strong></h2>

<h4 id="tensorboard-å¯è§†åŒ–">tensorboard å¯è§†åŒ–</h4>
<h6 id="æ˜¾ç¤ºç½‘ç»œç»“æ„">æ˜¾ç¤ºç½‘ç»œç»“æ„</h6>
<p>ä½¿ç”¨tensorboardå¯è§†åŒ–æˆ‘ä»¬æ„å»ºçš„ç½‘ç»œï¼Œå¯ä»¥ç›´è§‚çš„æ˜¾ç¤ºå‡ºç¥ç»ç½‘ç»œçš„ç»“æ„ã€‚
<img src="https://morvanzhou.github.io/static/results/tensorflow/4_1_1.png" alt="1jpg" />
å¯ä»¥ç‚¹å¼€æ¯ä¸€ä¸ªlayeræŸ¥çœ‹layerå†…éƒ¨çš„å…ƒç´ ã€‚
<img src="https://morvanzhou.github.io/static/results/tensorflow/4_1_2.png" alt="2.jpg" /></p>

<p>å®ç°æ–¹å¼å°±æ˜¯åœ¨éœ€è¦æ˜¾ç¤ºçš„ä¹‹å‰æ·»åŠ <code class="language-plaintext highlighter-rouge">with tf.name_scope('**'):</code>,ä¾‹å¦‚ä¸Šé¢ä¾‹å­ä¸­æ·»åŠ å¦‚ä¸‹ï¼š</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from __future__ import print_function
import tensorflow as tf


def add_layer(inputs, in_size, out_size, activation_function=None):
    # add one more layer and return the output of this layer
    with tf.name_scope('layer'):
        with tf.name_scope('weights'):
            Weights = tf.Variable(tf.random_normal([in_size, out_size]), name='W')
        with tf.name_scope('biases'):
            biases = tf.Variable(tf.zeros([1, out_size]) + 0.1, name='b')
        with tf.name_scope('Wx_plus_b'):
            Wx_plus_b = tf.add(tf.matmul(inputs, Weights), biases)
        if activation_function is None:
            outputs = Wx_plus_b
        else:
            outputs = activation_function(Wx_plus_b, )
        return outputs


# define placeholder for inputs to network
with tf.name_scope('inputs'):
    xs = tf.placeholder(tf.float32, [None, 1], name='x_input')
    ys = tf.placeholder(tf.float32, [None, 1], name='y_input')

# add hidden layer
l1 = add_layer(xs, 1, 10, activation_function=tf.nn.relu)
# add output layer
prediction = add_layer(l1, 10, 1, activation_function=None)

# the error between prediciton and real data
with tf.name_scope('loss'):
    loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction),
                                        reduction_indices=[1]))

with tf.name_scope('train'):
    train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)

sess = tf.Session()

# tf.train.SummaryWriter soon be deprecated, use following
if int((tf.__version__).split('.')[1]) &lt; 12 and int((tf.__version__).split('.')[0]) &lt; 1:  # tensorflow version &lt; 0.12
    writer = tf.train.SummaryWriter('logs/', sess.graph)
else: # tensorflow version &gt;= 0.12
    writer = tf.summary.FileWriter("logs/", sess.graph)

# tf.initialize_all_variables() no long valid from
# 2017-03-02 if using tensorflow &gt;= 0.12
if int((tf.__version__).split('.')[1]) &lt; 12 and int((tf.__version__).split('.')[0]) &lt; 1:
    init = tf.initialize_all_variables()
else:
    init = tf.global_variables_initializer()
sess.run(init)
</code></pre></div></div>

<h6 id="æ˜¾ç¤ºè®­ç»ƒè¿‡ç¨‹">æ˜¾ç¤ºè®­ç»ƒè¿‡ç¨‹</h6>
<p>è®­ç»ƒè¿‡ç¨‹ä¸­çš„å‚æ•°å˜åŒ–ä¹Ÿå¯ä»¥é€šè¿‡tensorboardæ˜¾ç¤ºï¼Œå¸¸ç”¨çš„æ˜¯æ˜¾ç¤ºå‡ºæƒé‡çš„å˜åŒ–ï¼Œlossçš„å˜åŒ–ï¼Œå‡†ç¡®ç‡çš„å˜åŒ–ç­‰ç­‰ã€‚
losså˜åŒ–ï¼š
<img src="https://morvanzhou.github.io/static/results/tensorflow/4_2_3.png" alt="" />
layerå‚æ•°å˜åŒ–ï¼š
<img src="https://morvanzhou.github.io/static/results/tensorflow/4_2_4.png" alt="" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from __future__ import print_function
import tensorflow as tf
import numpy as np


def add_layer(inputs, in_size, out_size, n_layer, activation_function=None):
    # add one more layer and return the output of this layer
    layer_name = 'layer%s' % n_layer        #define a new var       å› ä¸ºä¸åŒçš„layerå±äºä¸åŒå˜é‡
    with tf.name_scope(layer_name):
        with tf.name_scope('weights'):
            Weights = tf.Variable(tf.random_normal([in_size, out_size]), name='W')
            tf.summary.histogram(layer_name + '/weights', Weights)      #ç»˜åˆ¶å˜é‡å›¾ï¼Œç¬¬ä¸€ä¸ªæ˜¯åå­—ï¼Œç¬¬äºŒä¸ªæ˜¯å›¾æ ‡è¦è®°å½•çš„å˜é‡
        with tf.name_scope('biases'):
            biases = tf.Variable(tf.zeros([1, out_size]) + 0.1, name='b')
            tf.summary.histogram(layer_name + '/biases', biases)
        with tf.name_scope('Wx_plus_b'):
            Wx_plus_b = tf.add(tf.matmul(inputs, Weights), biases)
        if activation_function is None:
            outputs = Wx_plus_b
        else:
            outputs = activation_function(Wx_plus_b, )
        tf.summary.histogram(layer_name + '/outputs', outputs)
    return outputs


# Make up some real data
x_data = np.linspace(-1, 1, 300)[:, np.newaxis]
noise = np.random.normal(0, 0.05, x_data.shape)
y_data = np.square(x_data) - 0.5 + noise

# define placeholder for inputs to network
with tf.name_scope('inputs'):
    xs = tf.placeholder(tf.float32, [None, 1], name='x_input')
    ys = tf.placeholder(tf.float32, [None, 1], name='y_input')

# add hidden layer
l1 = add_layer(xs, 1, 10, n_layer=1, activation_function=tf.nn.relu)
# add output layer
prediction = add_layer(l1, 10, 1, n_layer=2, activation_function=None)

# the error between prediciton and real data
with tf.name_scope('loss'):
    loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction),
                                        reduction_indices=[1]))
    tf.summary.scalar('loss', loss)

with tf.name_scope('train'):
    train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)

sess = tf.Session()
merged = tf.summary.merge_all()

writer = tf.summary.FileWriter("logs/", sess.graph)

init = tf.global_variables_initializer()
sess.run(init)

for i in range(1000):
    sess.run(train_step, feed_dict={xs: x_data, ys: y_data})
    if i % 50 == 0:
        result = sess.run(merged,
                          feed_dict={xs: x_data, ys: y_data})
        writer.add_summary(result, i)

</code></pre></div></div>
<h4 id="reference">reference</h4>
<p><a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/">è«å‡¡Python</a></p>
:ET